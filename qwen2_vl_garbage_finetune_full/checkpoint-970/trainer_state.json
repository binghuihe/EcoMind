{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0103126007089913,
      "grad_norm": 3.50616192817688,
      "learning_rate": 3.673469387755102e-05,
      "loss": 16.5598,
      "step": 10
    },
    {
      "epoch": 0.0206252014179826,
      "grad_norm": 6.583765506744385,
      "learning_rate": 7.755102040816327e-05,
      "loss": 14.884,
      "step": 20
    },
    {
      "epoch": 0.030937802126973897,
      "grad_norm": 4.1300883293151855,
      "learning_rate": 0.00011836734693877552,
      "loss": 10.0573,
      "step": 30
    },
    {
      "epoch": 0.0412504028359652,
      "grad_norm": 0.6586245894432068,
      "learning_rate": 0.00015918367346938776,
      "loss": 7.3468,
      "step": 40
    },
    {
      "epoch": 0.05156300354495649,
      "grad_norm": 0.21329601109027863,
      "learning_rate": 0.0002,
      "loss": 6.849,
      "step": 50
    },
    {
      "epoch": 0.061875604253947794,
      "grad_norm": 0.11740171909332275,
      "learning_rate": 0.000197828447339848,
      "loss": 6.7319,
      "step": 60
    },
    {
      "epoch": 0.0721882049629391,
      "grad_norm": 0.14109209179878235,
      "learning_rate": 0.000195656894679696,
      "loss": 6.7507,
      "step": 70
    },
    {
      "epoch": 0.0825008056719304,
      "grad_norm": 0.08429306745529175,
      "learning_rate": 0.00019348534201954397,
      "loss": 6.6067,
      "step": 80
    },
    {
      "epoch": 0.09281340638092168,
      "grad_norm": 0.10239952802658081,
      "learning_rate": 0.00019131378935939198,
      "loss": 6.6742,
      "step": 90
    },
    {
      "epoch": 0.10312600708991299,
      "grad_norm": 0.07835301011800766,
      "learning_rate": 0.00018914223669923996,
      "loss": 6.6164,
      "step": 100
    },
    {
      "epoch": 0.11343860779890429,
      "grad_norm": 0.022076958790421486,
      "learning_rate": 0.00018697068403908794,
      "loss": 6.677,
      "step": 110
    },
    {
      "epoch": 0.12375120850789559,
      "grad_norm": 0.07290682196617126,
      "learning_rate": 0.00018479913137893595,
      "loss": 6.7605,
      "step": 120
    },
    {
      "epoch": 0.13406380921688688,
      "grad_norm": 0.10371337831020355,
      "learning_rate": 0.00018262757871878393,
      "loss": 6.5999,
      "step": 130
    },
    {
      "epoch": 0.1443764099258782,
      "grad_norm": 0.04878341406583786,
      "learning_rate": 0.00018045602605863193,
      "loss": 6.6148,
      "step": 140
    },
    {
      "epoch": 0.15468901063486948,
      "grad_norm": 0.03138340637087822,
      "learning_rate": 0.0001782844733984799,
      "loss": 6.6985,
      "step": 150
    },
    {
      "epoch": 0.1650016113438608,
      "grad_norm": 0.05815007537603378,
      "learning_rate": 0.0001761129207383279,
      "loss": 6.6612,
      "step": 160
    },
    {
      "epoch": 0.17531421205285208,
      "grad_norm": 0.07124733179807663,
      "learning_rate": 0.00017394136807817592,
      "loss": 6.6627,
      "step": 170
    },
    {
      "epoch": 0.18562681276184337,
      "grad_norm": 0.0752134919166565,
      "learning_rate": 0.0001717698154180239,
      "loss": 6.6025,
      "step": 180
    },
    {
      "epoch": 0.19593941347083468,
      "grad_norm": 0.06942149251699448,
      "learning_rate": 0.00016959826275787188,
      "loss": 6.6969,
      "step": 190
    },
    {
      "epoch": 0.20625201417982597,
      "grad_norm": 0.08966328948736191,
      "learning_rate": 0.0001674267100977199,
      "loss": 6.5943,
      "step": 200
    },
    {
      "epoch": 0.2165646148888173,
      "grad_norm": 0.06345539540052414,
      "learning_rate": 0.00016525515743756787,
      "loss": 6.5777,
      "step": 210
    },
    {
      "epoch": 0.22687721559780857,
      "grad_norm": 0.06483936309814453,
      "learning_rate": 0.00016308360477741587,
      "loss": 6.5612,
      "step": 220
    },
    {
      "epoch": 0.23718981630679986,
      "grad_norm": 0.13921664655208588,
      "learning_rate": 0.00016091205211726385,
      "loss": 6.7068,
      "step": 230
    },
    {
      "epoch": 0.24750241701579118,
      "grad_norm": 0.08716368675231934,
      "learning_rate": 0.00015874049945711183,
      "loss": 6.6672,
      "step": 240
    },
    {
      "epoch": 0.2578150177247825,
      "grad_norm": 0.02678924798965454,
      "learning_rate": 0.00015656894679695984,
      "loss": 6.6948,
      "step": 250
    },
    {
      "epoch": 0.26812761843377375,
      "grad_norm": 0.09880245476961136,
      "learning_rate": 0.00015439739413680782,
      "loss": 6.664,
      "step": 260
    },
    {
      "epoch": 0.27844021914276507,
      "grad_norm": 0.08549448102712631,
      "learning_rate": 0.00015222584147665583,
      "loss": 6.5544,
      "step": 270
    },
    {
      "epoch": 0.2887528198517564,
      "grad_norm": 0.11256109178066254,
      "learning_rate": 0.0001500542888165038,
      "loss": 6.615,
      "step": 280
    },
    {
      "epoch": 0.29906542056074764,
      "grad_norm": 0.0863444060087204,
      "learning_rate": 0.0001478827361563518,
      "loss": 6.6294,
      "step": 290
    },
    {
      "epoch": 0.30937802126973896,
      "grad_norm": 0.04079759120941162,
      "learning_rate": 0.0001457111834961998,
      "loss": 6.595,
      "step": 300
    },
    {
      "epoch": 0.31969062197873027,
      "grad_norm": 0.0437614805996418,
      "learning_rate": 0.00014353963083604777,
      "loss": 6.6479,
      "step": 310
    },
    {
      "epoch": 0.3300032226877216,
      "grad_norm": 0.15071538090705872,
      "learning_rate": 0.00014136807817589578,
      "loss": 6.6306,
      "step": 320
    },
    {
      "epoch": 0.34031582339671285,
      "grad_norm": 0.04141073673963547,
      "learning_rate": 0.00013919652551574375,
      "loss": 6.5789,
      "step": 330
    },
    {
      "epoch": 0.35062842410570416,
      "grad_norm": 0.05273531377315521,
      "learning_rate": 0.00013702497285559176,
      "loss": 6.5853,
      "step": 340
    },
    {
      "epoch": 0.3609410248146955,
      "grad_norm": 0.08329952508211136,
      "learning_rate": 0.00013485342019543974,
      "loss": 6.5548,
      "step": 350
    },
    {
      "epoch": 0.37125362552368674,
      "grad_norm": 0.025064703077077866,
      "learning_rate": 0.00013268186753528772,
      "loss": 6.6292,
      "step": 360
    },
    {
      "epoch": 0.38156622623267805,
      "grad_norm": 0.0730857104063034,
      "learning_rate": 0.00013051031487513573,
      "loss": 6.5801,
      "step": 370
    },
    {
      "epoch": 0.39187882694166937,
      "grad_norm": 0.1265605390071869,
      "learning_rate": 0.0001283387622149837,
      "loss": 6.6366,
      "step": 380
    },
    {
      "epoch": 0.4021914276506606,
      "grad_norm": 0.09875743091106415,
      "learning_rate": 0.0001261672095548317,
      "loss": 6.6831,
      "step": 390
    },
    {
      "epoch": 0.41250402835965194,
      "grad_norm": 0.051270969212055206,
      "learning_rate": 0.00012399565689467972,
      "loss": 6.5817,
      "step": 400
    },
    {
      "epoch": 0.42281662906864326,
      "grad_norm": 0.07276566326618195,
      "learning_rate": 0.0001218241042345277,
      "loss": 6.6431,
      "step": 410
    },
    {
      "epoch": 0.4331292297776346,
      "grad_norm": 0.12630322575569153,
      "learning_rate": 0.00011965255157437569,
      "loss": 6.6642,
      "step": 420
    },
    {
      "epoch": 0.44344183048662583,
      "grad_norm": 0.10751968622207642,
      "learning_rate": 0.00011748099891422368,
      "loss": 6.5561,
      "step": 430
    },
    {
      "epoch": 0.45375443119561715,
      "grad_norm": 0.1229068711400032,
      "learning_rate": 0.00011530944625407168,
      "loss": 6.6712,
      "step": 440
    },
    {
      "epoch": 0.46406703190460846,
      "grad_norm": 0.049775633960962296,
      "learning_rate": 0.00011313789359391967,
      "loss": 6.67,
      "step": 450
    },
    {
      "epoch": 0.4743796326135997,
      "grad_norm": 0.01203247718513012,
      "learning_rate": 0.00011096634093376765,
      "loss": 6.5765,
      "step": 460
    },
    {
      "epoch": 0.48469223332259104,
      "grad_norm": 0.0695471465587616,
      "learning_rate": 0.00010879478827361564,
      "loss": 6.5608,
      "step": 470
    },
    {
      "epoch": 0.49500483403158235,
      "grad_norm": 0.04898145794868469,
      "learning_rate": 0.00010662323561346363,
      "loss": 6.633,
      "step": 480
    },
    {
      "epoch": 0.5053174347405737,
      "grad_norm": 0.019341258332133293,
      "learning_rate": 0.00010445168295331163,
      "loss": 6.5891,
      "step": 490
    },
    {
      "epoch": 0.515630035449565,
      "grad_norm": 0.04324010759592056,
      "learning_rate": 0.00010228013029315962,
      "loss": 6.6313,
      "step": 500
    },
    {
      "epoch": 0.5259426361585562,
      "grad_norm": 0.05138437822461128,
      "learning_rate": 0.0001001085776330076,
      "loss": 6.6051,
      "step": 510
    },
    {
      "epoch": 0.5362552368675475,
      "grad_norm": 0.030477475374937057,
      "learning_rate": 9.793702497285559e-05,
      "loss": 6.6376,
      "step": 520
    },
    {
      "epoch": 0.5465678375765388,
      "grad_norm": 0.11454916000366211,
      "learning_rate": 9.576547231270358e-05,
      "loss": 6.5674,
      "step": 530
    },
    {
      "epoch": 0.5568804382855301,
      "grad_norm": 0.10029168426990509,
      "learning_rate": 9.359391965255158e-05,
      "loss": 6.5215,
      "step": 540
    },
    {
      "epoch": 0.5671930389945214,
      "grad_norm": 0.10890457779169083,
      "learning_rate": 9.142236699239957e-05,
      "loss": 6.5745,
      "step": 550
    },
    {
      "epoch": 0.5775056397035128,
      "grad_norm": 0.022009264677762985,
      "learning_rate": 8.925081433224756e-05,
      "loss": 6.6924,
      "step": 560
    },
    {
      "epoch": 0.5878182404125041,
      "grad_norm": 0.07006771862506866,
      "learning_rate": 8.707926167209556e-05,
      "loss": 6.5349,
      "step": 570
    },
    {
      "epoch": 0.5981308411214953,
      "grad_norm": 0.07933265715837479,
      "learning_rate": 8.490770901194355e-05,
      "loss": 6.6677,
      "step": 580
    },
    {
      "epoch": 0.6084434418304866,
      "grad_norm": 0.06111579388380051,
      "learning_rate": 8.273615635179154e-05,
      "loss": 6.6327,
      "step": 590
    },
    {
      "epoch": 0.6187560425394779,
      "grad_norm": 0.1033952608704567,
      "learning_rate": 8.056460369163953e-05,
      "loss": 6.589,
      "step": 600
    },
    {
      "epoch": 0.6290686432484692,
      "grad_norm": 0.10060196369886398,
      "learning_rate": 7.839305103148751e-05,
      "loss": 6.6369,
      "step": 610
    },
    {
      "epoch": 0.6393812439574605,
      "grad_norm": 0.045649975538253784,
      "learning_rate": 7.62214983713355e-05,
      "loss": 6.5566,
      "step": 620
    },
    {
      "epoch": 0.6496938446664519,
      "grad_norm": 0.07442507147789001,
      "learning_rate": 7.40499457111835e-05,
      "loss": 6.6389,
      "step": 630
    },
    {
      "epoch": 0.6600064453754432,
      "grad_norm": 0.06475722044706345,
      "learning_rate": 7.187839305103149e-05,
      "loss": 6.659,
      "step": 640
    },
    {
      "epoch": 0.6703190460844344,
      "grad_norm": 0.03212081640958786,
      "learning_rate": 6.970684039087949e-05,
      "loss": 6.7025,
      "step": 650
    },
    {
      "epoch": 0.6806316467934257,
      "grad_norm": 0.0639815703034401,
      "learning_rate": 6.753528773072746e-05,
      "loss": 6.6828,
      "step": 660
    },
    {
      "epoch": 0.690944247502417,
      "grad_norm": 0.04614879563450813,
      "learning_rate": 6.536373507057546e-05,
      "loss": 6.6968,
      "step": 670
    },
    {
      "epoch": 0.7012568482114083,
      "grad_norm": 0.06581392139196396,
      "learning_rate": 6.319218241042345e-05,
      "loss": 6.581,
      "step": 680
    },
    {
      "epoch": 0.7115694489203996,
      "grad_norm": 0.09757980704307556,
      "learning_rate": 6.102062975027145e-05,
      "loss": 6.6645,
      "step": 690
    },
    {
      "epoch": 0.721882049629391,
      "grad_norm": 0.012110709212720394,
      "learning_rate": 5.884907709011944e-05,
      "loss": 6.4386,
      "step": 700
    },
    {
      "epoch": 0.7321946503383822,
      "grad_norm": 0.08642362803220749,
      "learning_rate": 5.6677524429967436e-05,
      "loss": 6.6327,
      "step": 710
    },
    {
      "epoch": 0.7425072510473735,
      "grad_norm": 0.14369940757751465,
      "learning_rate": 5.450597176981542e-05,
      "loss": 6.674,
      "step": 720
    },
    {
      "epoch": 0.7528198517563648,
      "grad_norm": 0.1205533817410469,
      "learning_rate": 5.2334419109663414e-05,
      "loss": 6.6785,
      "step": 730
    },
    {
      "epoch": 0.7631324524653561,
      "grad_norm": 0.08565563708543777,
      "learning_rate": 5.01628664495114e-05,
      "loss": 6.5828,
      "step": 740
    },
    {
      "epoch": 0.7734450531743474,
      "grad_norm": 0.03824853524565697,
      "learning_rate": 4.7991313789359393e-05,
      "loss": 6.6609,
      "step": 750
    },
    {
      "epoch": 0.7837576538833387,
      "grad_norm": 0.09231241792440414,
      "learning_rate": 4.5819761129207386e-05,
      "loss": 6.6868,
      "step": 760
    },
    {
      "epoch": 0.79407025459233,
      "grad_norm": 0.11263833940029144,
      "learning_rate": 4.364820846905538e-05,
      "loss": 6.7025,
      "step": 770
    },
    {
      "epoch": 0.8043828553013213,
      "grad_norm": 0.07134997844696045,
      "learning_rate": 4.147665580890337e-05,
      "loss": 6.6011,
      "step": 780
    },
    {
      "epoch": 0.8146954560103126,
      "grad_norm": 0.034781843423843384,
      "learning_rate": 3.930510314875136e-05,
      "loss": 6.6128,
      "step": 790
    },
    {
      "epoch": 0.8250080567193039,
      "grad_norm": 0.03388582542538643,
      "learning_rate": 3.713355048859935e-05,
      "loss": 6.7411,
      "step": 800
    },
    {
      "epoch": 0.8353206574282952,
      "grad_norm": 0.09989362210035324,
      "learning_rate": 3.4961997828447344e-05,
      "loss": 6.5121,
      "step": 810
    },
    {
      "epoch": 0.8456332581372865,
      "grad_norm": 0.0869840607047081,
      "learning_rate": 3.279044516829533e-05,
      "loss": 6.6022,
      "step": 820
    },
    {
      "epoch": 0.8559458588462778,
      "grad_norm": 0.09822285920381546,
      "learning_rate": 3.061889250814333e-05,
      "loss": 6.6035,
      "step": 830
    },
    {
      "epoch": 0.8662584595552691,
      "grad_norm": 0.07310100644826889,
      "learning_rate": 2.8447339847991315e-05,
      "loss": 6.6383,
      "step": 840
    },
    {
      "epoch": 0.8765710602642603,
      "grad_norm": 0.04934879392385483,
      "learning_rate": 2.6275787187839308e-05,
      "loss": 6.6281,
      "step": 850
    },
    {
      "epoch": 0.8868836609732517,
      "grad_norm": 0.04263211041688919,
      "learning_rate": 2.4104234527687298e-05,
      "loss": 6.6773,
      "step": 860
    },
    {
      "epoch": 0.897196261682243,
      "grad_norm": 0.016335755586624146,
      "learning_rate": 2.1932681867535287e-05,
      "loss": 6.6313,
      "step": 870
    },
    {
      "epoch": 0.9075088623912343,
      "grad_norm": 0.06299393624067307,
      "learning_rate": 1.976112920738328e-05,
      "loss": 6.6251,
      "step": 880
    },
    {
      "epoch": 0.9178214631002256,
      "grad_norm": 0.016443269327282906,
      "learning_rate": 1.758957654723127e-05,
      "loss": 6.7001,
      "step": 890
    },
    {
      "epoch": 0.9281340638092169,
      "grad_norm": 0.11817323416471481,
      "learning_rate": 1.5418023887079262e-05,
      "loss": 6.6771,
      "step": 900
    },
    {
      "epoch": 0.9384466645182082,
      "grad_norm": 0.06979790329933167,
      "learning_rate": 1.3246471226927253e-05,
      "loss": 6.6206,
      "step": 910
    },
    {
      "epoch": 0.9487592652271994,
      "grad_norm": 0.026155410334467888,
      "learning_rate": 1.1074918566775245e-05,
      "loss": 6.6287,
      "step": 920
    },
    {
      "epoch": 0.9590718659361908,
      "grad_norm": 0.09853591024875641,
      "learning_rate": 8.903365906623236e-06,
      "loss": 6.7115,
      "step": 930
    },
    {
      "epoch": 0.9693844666451821,
      "grad_norm": 0.15017755329608917,
      "learning_rate": 6.731813246471228e-06,
      "loss": 6.6631,
      "step": 940
    },
    {
      "epoch": 0.9796970673541734,
      "grad_norm": 0.07748985290527344,
      "learning_rate": 4.560260586319218e-06,
      "loss": 6.716,
      "step": 950
    },
    {
      "epoch": 0.9900096680631647,
      "grad_norm": 0.09095027297735214,
      "learning_rate": 2.3887079261672097e-06,
      "loss": 6.5863,
      "step": 960
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.009789800271391869,
      "learning_rate": 2.1715526601520088e-07,
      "loss": 6.6402,
      "step": 970
    }
  ],
  "logging_steps": 10,
  "max_steps": 970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4652765224347546e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
