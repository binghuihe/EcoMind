from unsloth import FastVisionModel
from peft import PeftModel
from PIL import Image
import os
import torch

# ================= é…ç½® =================
BASE_MODEL_PATH = "./Qwen2-VL-4bit"
ADAPTER_PATH = "qwen2_vl_garbage_finetune"
# ========================================

print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ï¼Œè¯·ç¨å€™...")
model, tokenizer = FastVisionModel.from_pretrained(
    BASE_MODEL_PATH,
    load_in_4bit = True,
    use_gradient_checkpointing = "unsloth",
)
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
FastVisionModel.for_inference(model)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

while True:
    print("\n" + "-"*50)
    # 1. è·å–è¾“å…¥
    user_input = input("ğŸ‘‰ è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ (ç›´æ¥æŠŠæ–‡ä»¶æ‹–è¿›æ¥ï¼Œè¾“å…¥ q é€€å‡º): ").strip()
    
    if user_input.lower() in ['q', 'exit', 'quit']:
        print("ğŸ‘‹ Bye!")
        break
        
    # å»é™¤æ‹–æ‹½å¯èƒ½äº§ç”Ÿçš„å¼•å·
    image_path = user_input.replace('"', '').replace("'", "").strip()
    
    # 2. è·¯å¾„ä¿®å¤ (WSL)
    if ":" in image_path and "\\" in image_path:
        try:
            drive, rest = os.path.splitdrive(image_path)
            # æš´åŠ›å­—ç¬¦ä¸²æ›¿æ¢
            clean_rest = rest.replace('\\', '/')
            image_path = f"/mnt/{drive[0].lower()}{clean_rest}"
        except:
            pass
            
    # 3. æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(image_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {image_path}")
        continue
        
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡è¯»å–å¤±è´¥: {e}")
        continue

    # 4. æ¨ç†
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": "What kind of garbage is this?\nè¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„åƒåœ¾ç±»åˆ«ã€‚"}
            ]
        }
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(image, text, add_special_tokens=False, return_tensors="pt").to("cuda")

    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.1)
    
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    final_answer = response.split("assistant")[-1].strip()

    print(f"ğŸ¤– è¯†åˆ«ç»“æœ: \033[1;32m{final_answer}\033[0m")