from unsloth import FastVisionModel
import torch
from datasets import load_dataset
from transformers import Trainer, TrainingArguments
from PIL import Image
import os

# ================= é…ç½®åŒºåŸŸ (ç»å¯¹ç¨³å¥ç‰ˆ) =================
DATA_FILE = "garbage_data_train.jsonl" 
MODEL_PATH = "./Qwen2-VL-4bit"
OUTPUT_DIR = "qwen2_vl_garbage_finetune_full"

MAX_SEQ_LENGTH = 1024

# âœ… æ ¸å¿ƒé…ç½®ï¼šBatch=1 + ç´¯ç§¯16æ­¥ = æ—¢çœæ˜¾å­˜åˆè·‘å¾—å¿«
BATCH_SIZE = 1 
GRAD_ACCUMULATION = 16 

# âœ… æ ¸å¿ƒé…ç½®ï¼šä½¿ç”¨ Epoch ç­–ç•¥
# è®¾ä¸º 1 ä»£è¡¨æŠŠæ‰€æœ‰æ•°æ®çœ‹ 1 éï¼ˆé€šå¸¸è¶³å¤Ÿè®© LoRA å­¦ä¼šåˆ†ç±»è§„åˆ™ï¼‰
# å¦‚æœä½ è§‰å¾—æ•ˆæœä¸å¤Ÿå¥½ï¼Œå¯ä»¥æ”¹æˆ 3
NUM_TRAIN_EPOCHS = 1
# =======================================================

# 1. åŠ è½½æœ¬åœ°æ¨¡å‹
print("ğŸš€ [1/6] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹...")
model, tokenizer = FastVisionModel.from_pretrained(
    MODEL_PATH,
    load_in_4bit = True,
    use_gradient_checkpointing = "unsloth",
)

# 2. æŒ‚è½½ LoRA é€‚é…å™¨
print("ğŸ”— [2/6] æ­£åœ¨é…ç½® LoRA å‚æ•°...")
model = FastVisionModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

# 3. åŠ è½½æ•°æ®
print(f"ğŸ“‚ [3/6] æ­£åœ¨è¯»å–æ•°æ®: {DATA_FILE}")
dataset = load_dataset("json", data_files = DATA_FILE, split = "train")
print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±åŒ…å« {len(dataset)} æ¡æ•°æ®ã€‚")

# 4. è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ (é›†æˆ WSL ä¿®å¤ + æ‰‹åŠ¨ Prompt)
class Qwen2VLDataCollator:
    def __init__(self, processor):
        self.processor = processor
    
    def __call__(self, examples):
        texts = []
        images = []
        
        for example in examples:
            original_messages = example["messages"]
            
            # --- A. æå–ä¿¡æ¯ ---
            first_content = original_messages[0]["content"]
            image_entry = next((item for item in first_content if item['type'] == 'image'), None)
            text_entry = next((item for item in first_content if item['type'] == 'text'), None)
            # å…¼å®¹å¤šè½®å¯¹è¯ç»“æ„ï¼Œæå– Assistant çš„å›ç­”
            assist_entry = original_messages[1]["content"][0] if len(original_messages) > 1 else None
            
            if not image_entry or not text_entry or not assist_entry:
                continue

            raw_path = image_entry["image"]
            user_text = text_entry["text"]
            answer_text = assist_entry["text"] if isinstance(assist_entry, dict) else str(assist_entry)
            
            # --- B. WSL è·¯å¾„è‡ªåŠ¨ä¿®å¤ ---
            image_path = raw_path
            # å¦‚æœæ˜¯ Windows æ ¼å¼ (C:\...) ä¸”è¿è¡Œåœ¨ WSL ç¯å¢ƒ
            if ":" in raw_path and "\\" in raw_path:
                try:
                    parts = raw_path.split(":", 1)
                    if len(parts) == 2:
                        drive_letter = parts[0].lower()
                        clean_path = parts[1].replace('\\', '/')
                        image_path = f"/mnt/{drive_letter}{clean_path}"
                except:
                    pass
            
            # --- C. è¯»å–å›¾ç‰‡ ---
            try:
                image = Image.open(image_path).convert("RGB")
                images.append(image)
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è¯»å–å›¾ç‰‡: {image_path} (å·²è·³è¿‡)")
                continue
            
            # --- D. æ„é€  Prompt (Unsloth å®˜æ–¹æ¨èæ ¼å¼) ---
            # æ‰‹åŠ¨æ‹¼æ¥é¿å… <|image_pad|> é‡å¤æˆ–ä¸¢å¤±
            prompt = f"<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>{user_text}<|im_end|>\n<|im_start|>assistant\n{answer_text}<|im_end|>"
            texts.append(prompt)
            
        if len(images) == 0:
            return None # é‡åˆ°åæ•°æ®æ—¶è¿”å› Noneï¼ŒTrainer ä¼šè‡ªåŠ¨è·³è¿‡

        # --- E. æ‰¹é‡ç¼–ç  ---
        batch = self.processor(
            text=texts,
            images=images,
            return_tensors="pt",
            padding=True,
        )
        
        # å¤„ç†æ ‡ç­¾ (Maskæ‰ padding éƒ¨åˆ†)
        labels = batch["input_ids"].clone()
        if self.processor.tokenizer.pad_token_id is not None:
            labels[labels == self.processor.tokenizer.pad_token_id] = -100
        batch["labels"] = labels
        
        return batch

my_collator = Qwen2VLDataCollator(tokenizer)

# 5. è®¾ç½®è®­ç»ƒå‚æ•°
print("âš™ï¸ [4/6] é…ç½®è®­ç»ƒå‚æ•°...")
training_args = TrainingArguments(
    output_dir = OUTPUT_DIR,
    per_device_train_batch_size = BATCH_SIZE, 
    gradient_accumulation_steps = GRAD_ACCUMULATION,
    
    # âœ… å…³é”®ï¼šæŒ‰ Epoch è®­ç»ƒï¼Œä¸æŒ‰ Step
    num_train_epochs = NUM_TRAIN_EPOCHS,
    
    warmup_ratio = 0.05, 
    learning_rate = 2e-4,
    fp16 = not torch.cuda.is_bf16_supported(),
    bf16 = torch.cuda.is_bf16_supported(),
    
    logging_steps = 10,
    save_strategy = "epoch", # æ¯è·‘å®Œä¸€è½®ä¿å­˜ä¸€æ¬¡
    
    optim = "adamw_8bit",
    seed = 3407,
    remove_unused_columns = False, 
    label_names = ["labels"],
    report_to = "none", # ä¸ä¸Šä¼  wandbï¼Œçº¯æœ¬åœ°
)

# 6. å¼€å§‹è®­ç»ƒ
print(f"ğŸ”¥ [5/6] å¼€å§‹å¾®è°ƒï¼é¢„è®¡å°†è·‘å®Œ {NUM_TRAIN_EPOCHS} ä¸ª Epoch...")
trainer = Trainer(
    model = model,
    train_dataset = dataset,
    data_collator = my_collator,
    args = training_args,
)

trainer_stats = trainer.train()

# 7. ä¿å­˜ç»“æœ
print(f"ğŸ’¾ [6/6] è®­ç»ƒå®Œæˆï¼æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ° {OUTPUT_DIR} ...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("âœ… æ‰€æœ‰æ­¥éª¤é¡ºåˆ©å®Œæˆï¼è¯·ä½¿ç”¨ inference_rag.py è¿›è¡Œæµ‹è¯•ã€‚")