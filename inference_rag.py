from unsloth import FastVisionModel
from peft import PeftModel
from PIL import Image
import os
import torch
import chromadb
from chromadb.utils import embedding_functions

# ================= RAG é…ç½® =================
print("ğŸ“š æ­£åœ¨åŠ è½½ RAG å‘é‡çŸ¥è¯†åº“...")
chroma_client = chromadb.PersistentClient(path="./rag_db")
emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
collection = chroma_client.get_collection(name="garbage_knowledge", embedding_function=emb_fn)

# ================= æ¨¡å‹é…ç½® =================
BASE_MODEL_PATH = "./Qwen2-VL-4bit"
ADAPTER_PATH = "qwen2_vl_garbage_finetune_full" 
if not os.path.exists(ADAPTER_PATH):
    ADAPTER_PATH = "qwen2_vl_garbage_finetune"

print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ EcoMind (é€‚é…å™¨: {ADAPTER_PATH})...")
model, tokenizer = FastVisionModel.from_pretrained(
    BASE_MODEL_PATH,
    load_in_4bit = True,
    use_gradient_checkpointing = "unsloth",
)
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
FastVisionModel.for_inference(model)
print("âœ… ç³»ç»Ÿå°±ç»ªï¼")

# ================= äº¤äº’å¾ªç¯ =================
while True:
    print("\n" + "="*60)
    user_input = input("ğŸ“¸ è¯·æ‹–å…¥åƒåœ¾å›¾ç‰‡ (q é€€å‡º): ").strip()
    if user_input.lower() in ['q', 'exit']: break
    
    image_path = user_input.replace('"', '').replace("'", "").strip()
    if ":" in image_path and "\\" in image_path:
        try:
            drive, rest = os.path.splitdrive(image_path)
            clean_rest = rest.replace('\\', '/')
            image_path = f"/mnt/{drive[0].lower()}{clean_rest}"
        except: pass
            
    if not os.path.exists(image_path):
        print(f"âŒ å›¾ç‰‡ä¸å­˜åœ¨")
        continue
    try:
        image = Image.open(image_path).convert("RGB")
    except: continue

    # --- 1. è§†è§‰è¯†åˆ« ---
    print("ğŸ¤– 1. è§†è§‰æ¨¡å‹æ­£åœ¨åˆ†æ...")
    messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": "What kind of garbage is this?"}]}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(image, text, add_special_tokens=False, return_tensors="pt").to("cuda")
    
    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.1)
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    category_result = response.split("assistant")[-1].strip()
    
    print(f"ğŸ‘ï¸ è¯†åˆ«æ ‡ç­¾: \033[1;33m{category_result}\033[0m")

    # --- 2. è¯­ä¹‰æ£€ç´¢ (å¢å¼ºç‰ˆ) ---
    print("ğŸ§  2. æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“ (è·å– Top-2 ç»“æœ)...")
    
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šn_results=2ï¼Œé˜²æ­¢ç¬¬ä¸€ååŒ¹é…é”™è¯¯
    results = collection.query(
        query_texts=[category_result], 
        n_results=2 
    )
    
    print("\n" + "-"*20 + " ğŸŒ EcoMind ä¸“å®¶åˆ†ææŠ¥å‘Š " + "-"*20)
    
    if results['documents']:
        # éå†æ‰€æœ‰æ£€ç´¢åˆ°çš„ç»“æœ
        for i, doc in enumerate(results['documents'][0]):
            source = results['metadatas'][0][i]['source']
            print(f"\nğŸ“„ [ç›¸å…³çŸ¥è¯† {i+1}] (æ¥æº: {source})")
            print(f"\033[0;32m{doc}\033[0m") # ç»¿è‰²æ‰“å°çŸ¥è¯†å†…å®¹
            print("-" * 40)
    else:
        print("æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ã€‚")
    
    print("=" * 60)