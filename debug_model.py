import torch
from unsloth import FastVisionModel
from PIL import Image
import os

# 1. æ¨¡æ‹ŸåŠ è½½æ¨¡å‹
print("ğŸ”„ [1/4] æ­£åœ¨åŠ è½½æ¨¡å‹...")
model, tokenizer = FastVisionModel.from_pretrained(
    "./Qwen2-VL-4bit",
    load_in_4bit=True,
)
FastVisionModel.for_inference(model)

# 2. æ¨¡æ‹ŸåŠ è½½å›¾ç‰‡ (è‡ªåŠ¨æ‰¾ä¸€å¼ ä½ ä¸Šä¼ è¿‡çš„å›¾ç‰‡ï¼Œæˆ–è€…ç”±ä½ æŒ‡å®š)
# è¯·ç¡®ä¿ä½ çš„ç›®å½•ä¸‹æœ‰ä¸€å¼  jpg å›¾ç‰‡ï¼Œè¿™é‡Œæˆ‘å†™æ­»ä¸€ä¸ªåå­—ï¼Œä½ éœ€è¦æ”¹æˆä½ çœŸå®å­˜åœ¨çš„å›¾ç‰‡å
image_path = '/home/hui/ocr_gb/wx_chatou1.jpg'
# â–²â–²â–² æ³¨æ„ï¼šå¦‚æœæ²¡æœ‰ test.jpgï¼Œä»£ç ä¼šæŠ¥é”™ï¼Œè¯·æŠŠè¿™é‡Œæ”¹æˆä½ æ–‡ä»¶å¤¹é‡Œéšä¾¿ä¸€å¼ åƒåœ¾å›¾ç‰‡çš„åå­— â–²â–²â–²

if not os.path.exists(image_path):
    # å°è¯•è‡ªåŠ¨æ‰¾ä¸€å¼ 
    files = [f for f in os.listdir('.') if f.endswith('.jpg') or f.endswith('.png')]
    if files:
        image_path = files[0]
        print(f"âš ï¸ æœªæ‰¾åˆ°æŒ‡å®šå›¾ç‰‡ï¼Œè‡ªåŠ¨ä½¿ç”¨: {image_path}")
    else:
        print("âŒ é”™è¯¯ï¼šå½“å‰ç›®å½•ä¸‹æ²¡æœ‰å›¾ç‰‡ï¼Œæ— æ³•æµ‹è¯•ï¼")
        exit()

print(f"ğŸ“¸ [2/4] æ­£åœ¨è¯»å–å›¾ç‰‡: {image_path}")
image = Image.open(image_path).convert("RGB")

# 3. æ„é€ æœ€ç®€å•çš„ Prompt
print("ğŸ§  [3/4] æ­£åœ¨å°è¯•æ¨ç†...")
messages = [
    {"role": "user", "content": [
        {"type": "image"},
        {"type": "text", "text": "è¯¦ç»†æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ï¼Œå¹¶å‘Šè¯‰æˆ‘å®ƒå±äºä»€ä¹ˆåƒåœ¾ï¼Ÿ"}
    ]}
]

text_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(image, text_prompt, add_special_tokens=False, return_tensors="pt").to("cuda")

# 4. ç”Ÿæˆ (å¼ºåˆ¶æ‰“å°æ¯ä¸€ä¸ª token)
print("ğŸš€ [4/4] å¼€å§‹ç”Ÿæˆ (è¯·ç›¯ç€ä¸‹é¢)...")

with torch.no_grad():
    outputs = model.generate(
        **inputs, 
        max_new_tokens=256, 
        do_sample=False # ä½¿ç”¨è´ªå©ªè§£ç ï¼Œæœ€ç¨³å®š
    )

# 5. è§£ç 
print("-" * 30)
print("RAW OUTPUT (åŸå§‹ Token ID):")
print(outputs)
print("-" * 30)
decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=False) # ä¸è·³è¿‡ç‰¹æ®Šå­—ç¬¦ï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰ <|endoftext|>
print("DECODED TEXT (è§£ç æ–‡æœ¬):")
print(decoded_text)
print("-" * 30)

if "assistant" in decoded_text:
    print("âœ… æˆåŠŸï¼æå–å‡ºçš„å›ç­”ï¼š")
    print(decoded_text.split("assistant")[-1])
else:
    print("âŒ å¤±è´¥ï¼æ¨¡å‹è¾“å‡ºäº†å†…å®¹ï¼Œä½†æ ¼å¼ä¸å¯¹ (æ‰¾ä¸åˆ° 'assistant' æ ‡è®°)ã€‚")