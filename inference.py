from unsloth import FastVisionModel
from peft import PeftModel
from PIL import Image
import os
import torch

# ================= é…ç½®åŒºåŸŸ =================
BASE_MODEL_PATH = "./Qwen2-VL-4bit"
ADAPTER_PATH = "qwen2_vl_garbage_finetune"

# ğŸ‘‡ è¿™é‡Œçš„è·¯å¾„ä¸éœ€è¦æ”¹ï¼Œä¸‹é¢çš„ä»£ç ä¼šè‡ªåŠ¨ä¿®
TEST_IMAGE_PATH = r"C:\Users\Administrator\Desktop\garbage_classification\battery\battery10.jpg"
# ===========================================

print("ğŸš€ [1/3] æ­£åœ¨åŠ è½½æœ¬åœ°åŸºç¡€æ¨¡å‹ (ç»å¯¹ä¸è”ç½‘)...")
model, tokenizer = FastVisionModel.from_pretrained(
    BASE_MODEL_PATH,
    load_in_4bit = True,
    use_gradient_checkpointing = "unsloth",
)

print(f"ğŸ”— [2/3] æ­£åœ¨æŒ‚è½½ LoRA é€‚é…å™¨: {ADAPTER_PATH} ...")
model = PeftModel.from_pretrained(model, ADAPTER_PATH)
FastVisionModel.for_inference(model) 

# ================= è·¯å¾„ä¿®å¤ (æš´åŠ›å­—ç¬¦ä¸²ç‰ˆ) =================
image_path = TEST_IMAGE_PATH

# åªè¦åŒ…å« : å’Œ \ å°±è®¤ä¸ºæ˜¯ Windows è·¯å¾„ï¼Œå¼ºåˆ¶è½¬æ¢
if ":" in image_path and "\\" in image_path:
    try:
        # æ‰‹åŠ¨åˆ‡å‰²ï¼Œä¸ä¾èµ– os.path.splitdrive
        # "C:\Users..." -> ["C", "\Users..."]
        parts = image_path.split(":", 1)
        
        if len(parts) == 2:
            drive_letter = parts[0].lower() # æ‹¿åˆ° "c"
            rest_of_path = parts[1]         # æ‹¿åˆ° "\Users..."
            
            # æ›¿æ¢åæ–œæ 
            clean_rest = rest_of_path.replace('\\', '/')
            
            # æ‹¼è£…æˆ /mnt/c/Users...
            image_path = f"/mnt/{drive_letter}{clean_rest}"
            
    except Exception as e:
        print(f"âš ï¸ è·¯å¾„è½¬æ¢å‡ºé”™: {e}")

print(f"ğŸ–¼ï¸ [3/3] æ­£åœ¨æµ‹è¯•å›¾ç‰‡: {image_path}")

try:
    image = Image.open(image_path).convert("RGB")
    print("âœ… å›¾ç‰‡è¯»å–æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚\né”™è¯¯: {e}")
    exit()

# ================= æ„é€  Prompt å¹¶æ¨ç† =================
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "What kind of garbage is this?\nè¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„åƒåœ¾ç±»åˆ«ã€‚"}
        ]
    }
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(
    image,
    text,
    add_special_tokens=False,
    return_tensors="pt",
).to("cuda")

print("ğŸ¤– æ¨¡å‹æ­£åœ¨æ€è€ƒ...")
outputs = model.generate(
    **inputs, 
    max_new_tokens=128, 
    use_cache=True,
    temperature=0.1, 
)

response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
final_answer = response.split("assistant")[-1].strip()

print("\n" + "="*30)
print(f"ğŸ”® é¢„æµ‹ç»“æœ: {final_answer}")
print("="*30)