import streamlit as st
from unsloth import FastVisionModel
from PIL import Image
import torch

st.title("ğŸ› ï¸ æç®€ Debug æ¨¡å¼")

# 1. åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    model, tokenizer = FastVisionModel.from_pretrained(
        "./Qwen2-VL-4bit",
        load_in_4bit=True,
    )
    FastVisionModel.for_inference(model)
    return model, tokenizer

try:
    model, tokenizer = load_model()
    st.success("æ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    st.error(f"æ¨¡å‹æŒ‚äº†: {e}")
    st.stop()

# 2. ä¸Šä¼ 
uploaded_file = st.file_uploader("ä¼ å¼ å›¾è¯•è¯•", type=["jpg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, width=300)

    # 3. èŠå¤©æ¡† (ç›´æ¥æµ‹è¯•ç”Ÿæˆ)
    user_input = st.text_input("é—®ç‚¹ä»€ä¹ˆ (ä¾‹å¦‚: è¿™æ˜¯ä»€ä¹ˆ?)", "è¿™æ˜¯ä»€ä¹ˆ?")
    
    if st.button("å‘é€æµ‹è¯•"):
        # æ„é€  Prompt
        messages = [
            {"role": "user", "content": [
                {"type": "image"},
                {"type": "text", "text": user_input}
            ]}
        ]
        
        text_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(image, text_prompt, add_special_tokens=False, return_tensors="pt").to("cuda")

        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        st.write("æ­£åœ¨ç”Ÿæˆ Token...")
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=128)
            
        # æš´åŠ›æ˜¾ç¤ºæ‰€æœ‰è¾“å‡ºï¼Œä¸åš split
        raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        st.write("--- åŸå§‹è¾“å‡º ---")
        st.code(raw_text) # æŠŠåŸæœ¬æ¨¡å‹åå‡ºæ¥çš„æ‰€æœ‰ä¸œè¥¿éƒ½æ˜¾ç¤ºå‡ºæ¥
        
        st.write("--- å°è¯•æå– ---")
        if "assistant" in raw_text:
            st.success(raw_text.split("assistant")[-1])
        else:
            st.warning("æ²¡æ‰¾åˆ° assistant æ ‡è®°ï¼Œç›´æ¥æ˜¾ç¤ºæœ€åéƒ¨åˆ†:")
            st.info(raw_text)